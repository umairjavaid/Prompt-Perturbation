{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a24c4c",
   "metadata": {},
   "source": [
    "# App Review Analysis: Evaluating Foundation Model Robustness\n",
    "\n",
    "This notebook analyzes how different prompt formulations affect foundation models' interpretation of app store reviews. We'll evaluate model robustness across various tasks including sentiment analysis, feature request identification, and bug report detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947c87b",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "The project consists of the following components:\n",
    "\n",
    "1. `data_loader.py`: Handles loading and preprocessing of app store review datasets\n",
    "2. `prompt_generator.py`: Creates base prompts and variants for review analysis\n",
    "3. `model_runner.py`: Executes foundation models with different prompts\n",
    "4. `evaluator.py`: Measures consistency and accuracy across prompt variants\n",
    "5. `visualizer.py`: Generates visualizations of model robustness metrics\n",
    "\n",
    "Each module is designed to be modular and reusable for different types of review analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = Path.cwd().parent / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Import project modules\n",
    "from utils.data_loader import AppReviewDataset\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f69affe",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll load a sample dataset of app store reviews using our `AppReviewDataset` class. The dataset will be preprocessed to ensure consistency and remove any invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ee585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset loader\n",
    "data_dir = Path.cwd().parent / 'data'\n",
    "dataset = AppReviewDataset(data_dir)\n",
    "\n",
    "# Load sample dataset (50 reviews)\n",
    "reviews_df = dataset.get_reviews(n_samples=50)\n",
    "\n",
    "# Display basic statistics\n",
    "stats = dataset.get_statistics()\n",
    "print(\"Dataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display first few reviews\n",
    "print(\"\\nSample Reviews:\")\n",
    "display(reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ddd7a",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "\n",
    "Let's validate our dataset to ensure it meets our requirements:\n",
    "1. Each review should have non-empty text\n",
    "2. Ratings should be within valid range (1-5)\n",
    "3. Dates should be properly formatted\n",
    "4. No duplicate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Validate the dataset and return validation results.\"\"\"\n",
    "    results = {\n",
    "        'total_reviews': len(df),\n",
    "        'empty_reviews': df['text'].isna().sum(),\n",
    "        'invalid_ratings': df[~df['rating'].between(1, 5)].shape[0],\n",
    "        'invalid_dates': df['date'].isna().sum(),\n",
    "        'duplicate_reviews': df.duplicated(subset=['text']).sum()\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_dataset(reviews_df)\n",
    "print(\"Validation Results:\")\n",
    "for key, value in validation_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df10621",
   "metadata": {},
   "source": [
    "## Prompt Generation and Analysis\n",
    "\n",
    "Now we'll explore different ways to phrase prompts for analyzing app store reviews. We'll create multiple variants for each task (sentiment analysis, feature request detection, and bug report identification) to test model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompt_generator import PromptGenerator\n",
    "\n",
    "# Initialize the prompt generator\n",
    "prompt_gen = PromptGenerator()\n",
    "\n",
    "# Display available tasks and styles\n",
    "print(\"Available tasks:\", prompt_gen.get_task_list())\n",
    "print(\"Available prompt styles:\", prompt_gen.get_style_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore prompt variants for a sample review\n",
    "sample_review = reviews_df.iloc[0]['text']\n",
    "\n",
    "# Generate prompts for each task\n",
    "for task in prompt_gen.get_task_list():\n",
    "    print(f\"\\n=== {task.upper()} ANALYSIS PROMPTS ===\")\n",
    "    variants = prompt_gen.get_prompt_variants(task, sample_review)\n",
    "    \n",
    "    for variant in variants:\n",
    "        print(f\"\\nStyle: {variant['style']}\")\n",
    "        print(\"Prompt:\")\n",
    "        print(variant['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212ae71",
   "metadata": {},
   "source": [
    "As we can see above, we have different prompt variants for each analysis task. The variants differ in:\n",
    "\n",
    "1. Formality level (formal vs casual)\n",
    "2. Structure (free-form vs structured)\n",
    "3. Level of detail in instructions\n",
    "4. Output format specification\n",
    "\n",
    "These variations will help us evaluate how sensitive the models are to prompt phrasing and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c48aba",
   "metadata": {},
   "source": [
    "## Model Execution and Result Collection\n",
    "\n",
    "In this section, we'll run our foundation models (OpenAI and Hugging Face) with different prompt variants. We'll collect and store the results for later analysis of model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23101b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from utils.model_runner import ModelRunner\n",
    "\n",
    "# Initialize the model runner\n",
    "model_runner = ModelRunner()\n",
    "\n",
    "# Display available models\n",
    "print(\"Available Models:\")\n",
    "for provider, models in model_runner.get_available_models().items():\n",
    "    print(f\"\\n{provider.upper()}:\")\n",
    "    for model in models:\n",
    "        print(f\"- {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_reviews_with_variants(reviews, n_samples=5):\n",
    "    \"\"\"Analyze a sample of reviews with different prompt variants.\"\"\"\n",
    "    results = []\n",
    "    sample_reviews = reviews.sample(n=n_samples)\n",
    "    \n",
    "    for _, review in sample_reviews.iterrows():\n",
    "        review_results = {'review_id': review['review_id'], 'text': review['text'], 'tasks': {}}\n",
    "        \n",
    "        for task in prompt_gen.get_task_list():\n",
    "            variants = prompt_gen.get_prompt_variants(task, review['text'])\n",
    "            task_results = []\n",
    "            \n",
    "            for variant in variants:\n",
    "                result = await model_runner.analyze_review(\n",
    "                    review['text'],\n",
    "                    variant['prompt'],\n",
    "                    task\n",
    "                )\n",
    "                task_results.append({\n",
    "                    'style': variant['style'],\n",
    "                    'prompt': variant['prompt'],\n",
    "                    'result': result\n",
    "                })\n",
    "                \n",
    "            review_results['tasks'][task] = task_results\n",
    "        results.append(review_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on a sample of reviews\n",
    "print(\"Analyzing reviews with different prompt variants...\")\n",
    "results = await analyze_reviews_with_variants(reviews_df, n_samples=5)\n",
    "\n",
    "# Save results for later analysis\n",
    "results_dir = Path.cwd().parent / 'data' / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_dir / 'analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nAnalysis complete. Results saved to {results_dir / 'analysis_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results for one review\n",
    "print(\"\\nSample Results Preview:\")\n",
    "sample_result = results[0]\n",
    "print(f\"Review: {sample_result['text']}\")\n",
    "\n",
    "for task, task_results in sample_result['tasks'].items():\n",
    "    print(f\"\\n=== {task.upper()} ANALYSIS ===\")\n",
    "    for variant_result in task_results:\n",
    "        print(f\"\\nStyle: {variant_result['style']}\")\n",
    "        if 'openai' in variant_result['result']:\n",
    "            print(\"OpenAI:\", variant_result['result']['openai']['response'])\n",
    "        if 'huggingface' in variant_result['result']:\n",
    "            print(\"HuggingFace:\", variant_result['result']['huggingface']['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1facf02",
   "metadata": {},
   "source": [
    "## Model Robustness Evaluation\n",
    "\n",
    "Now we'll evaluate the robustness of our models by analyzing:\n",
    "1. Consistency across prompt variants\n",
    "2. Agreement between different models\n",
    "3. Response stability\n",
    "4. Per-task and overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import ModelEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Load results from file\n",
    "results_path = Path.cwd().parent / 'data' / 'results' / 'analysis_results.json'\n",
    "with open(results_path) as f:\n",
    "    analysis_results = json.load(f)\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating model robustness...\")\n",
    "evaluation_results = evaluator.evaluate_results(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9325217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overall metrics\n",
    "print(\"\\nOverall Robustness Metrics:\")\n",
    "for metric, value in evaluation_results['overall_metrics'].items():\n",
    "    print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "# Create heatmap of task-specific metrics\n",
    "metrics_data = []\n",
    "for task, task_metrics in evaluation_results['per_task_metrics'].items():\n",
    "    for metric, value in task_metrics.items():\n",
    "        metrics_data.append({\n",
    "            'Task': task,\n",
    "            'Metric': metric,\n",
    "            'Value': value\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_pivot = metrics_df.pivot(index='Task', columns='Metric', values='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(metrics_pivot, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "plt.title('Model Robustness Metrics by Task')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60baacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-review metrics\n",
    "print(\"\\nPer-Review Analysis:\")\n",
    "for review_id, review_metrics in evaluation_results['per_review_metrics'].items():\n",
    "    review_text = next(\n",
    "        r['text'] for r in analysis_results if str(r['review_id']) == review_id\n",
    "    )\n",
    "    print(f\"\\nReview {review_id}: {review_text[:100]}...\")\n",
    "    \n",
    "    for task, metrics in review_metrics.items():\n",
    "        print(f\"  {task}: consistency = {metrics['consistency']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d0e5a",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Prompt Variation Consistency**: Measures how consistent model responses are across different prompt phrasings\n",
    "2. **Model Agreement**: Shows how well OpenAI and Hugging Face models agree on their predictions\n",
    "3. **Response Stability**: Indicates how stable the responses are when prompt style changes\n",
    "\n",
    "These metrics help us understand how robust the models are to changes in prompt formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582c441",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Let's create various visualizations to better understand the model robustness analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizer import ModelVisualizer\n",
    "\n",
    "# Initialize the visualizer\n",
    "vis_dir = Path.cwd().parent / 'data' / 'visualizations'\n",
    "visualizer = ModelVisualizer(vis_dir)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "# 1. Prompt impact heatmaps for each task\n",
    "for task in ['sentiment', 'feature_request', 'bug_report']:\n",
    "    visualizer.create_prompt_impact_heatmap(analysis_results, task)\n",
    "    \n",
    "# 2. Model agreement plot\n",
    "visualizer.plot_model_agreement(analysis_results)\n",
    "\n",
    "# 3. Consistency matrix\n",
    "visualizer.plot_consistency_matrix(evaluation_results)\n",
    "\n",
    "# 4. Robustness radar plot\n",
    "visualizer.plot_robustness_radar(evaluation_results)\n",
    "\n",
    "print(f\"\\nVisualizations saved to {vis_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21140424",
   "metadata": {},
   "source": [
    "### Interpretation of Visualizations\n",
    "\n",
    "1. **Prompt Impact Heatmaps**: Show how different prompt styles affect response characteristics for each task\n",
    "2. **Model Agreement Plot**: Visualizes the level of agreement between OpenAI and Hugging Face models\n",
    "3. **Consistency Matrix**: Displays the consistency of model behavior across different tasks and metrics\n",
    "4. **Robustness Radar**: Provides a high-level view of model robustness across multiple dimensions\n",
    "\n",
    "These visualizations help identify:\n",
    "- Which prompt styles lead to more consistent results\n",
    "- Where models tend to agree or disagree\n",
    "- Tasks that are more sensitive to prompt variations\n",
    "- Overall robustness patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
